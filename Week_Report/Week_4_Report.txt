Week 4 Log – CAP6415_F25_project-Sign_Language_Detection_Using_OpenCV

At the beginning of this week I revisited the dataset design based on the professor’s feedback that a realistic sign language project should cover more than just a few gestures. Originally I was focusing only on 4 “greeting” signs, but I extended the vocabulary to 12 everyday commands (best, busy, careful, eat, fight, god, good, great, hand, hello, here, home). For each new sign I organized the corresponding image folders and verified that the annotations and class labels are consistent with the project structure.

After finalizing this expanded dataset I focused on training the first deep learning baseline for sign language classification. I used the cleaned and labeled dataset (12 gesture classes) and connected it to the main training script train_sign_language.py. I configured the script to read data from the “data” folder and to save all outputs in an “output_model” directory. The model architecture is based on EfficientNetB0. In this first experiment I kept the EfficientNet backbone frozen and only trained the final dense layer for 30 epochs with early stopping and a small learning rate. Class weights were also computed automatically to reduce the impact of small class-imbalance.

The training pipeline successfully created training and validation datasets (288 train images and 72 validation images), ran the training loop, and generated several artifacts: final_model.keras, best_model.keras, label_map.json, accuracy and loss curves (PNG), a confusion matrix, and a classification_report.txt file.

The current model accuracy on the validation set is still low (around 5–6%), and the confusion matrix shows that the network is mainly predicting a single class. However, this run confirms that the full TensorFlow + Keras pipeline is working correctly on my machine with the enlarged set of 12 signs. In the next week I plan to improve the performance by unfreezing part of the EfficientNet backbone, adjusting the learning rate, and possibly collecting a few more samples per gesture if needed.
